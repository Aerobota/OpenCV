/*
THIS SOFTWARE IS PROVIDED BY COPYRIGHT HOLDERS ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE FREEBSD PROJECT OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
*/

#include "QVideoDecoder.h"
#include <limits.h>
#include <stdint.h>
#include "ffmpeg.h"

QVideoDecoder::QVideoDecoder()
{
    InitVars();
    initCodec();
}

QVideoDecoder::QVideoDecoder(QString file)
{
    InitVars();
    initCodec();

    ok = openFile(file.toStdString().c_str());
}

QVideoDecoder::~QVideoDecoder()
{
    close();
}

void QVideoDecoder::InitVars()
{
    ok=false;
    pFormatCtx=0;
    pCodecCtx=0;
    pCodec=0;
    pFrame=0;
    pFrameRGB=0;
    buffer=0;
    img_convert_ctx=0;
}

void QVideoDecoder::close()
{
    if(!ok)
        return;

    // Free the RGB image
    if(buffer)
        delete [] buffer;

    // Free the YUV frame
    if(pFrame)
        av_free(pFrame);

    // Free the RGB frame
    if(pFrameRGB)
        av_free(pFrameRGB);

    // Close the codec
    if(pCodecCtx)
        avcodec_close(pCodecCtx);

    // Close the video file
    if(pFormatCtx)
        av_close_input_file(pFormatCtx);

    InitVars();
}


bool QVideoDecoder::initCodec()
{
    ffmpeg::avcodec_init();
    ffmpeg::avcodec_register_all();
    ffmpeg::av_register_all();

    printf("License: %s\n",ffmpeg::avformat_license());
    printf("AVCodec version %d\n",ffmpeg::avformat_version());
    printf("AVFormat configuration: %s\n",ffmpeg::avformat_configuration());

    return true;
}

bool QVideoDecoder::openFile(QString filename)
{
    // Close last video..
    close();

    LastLastFrameTime=INT_MIN;       // Last last must be small to handle the seek well
    LastFrameTime=0;
    LastLastFrameNumber=INT_MIN;
    LastFrameNumber=0;
    DesiredFrameTime=DesiredFrameNumber=0;
    LastFrameOk=false;

    // Open video file
    if(av_open_input_file(&pFormatCtx, filename.toStdString().c_str(), NULL, 0, NULL)!=0)
        return false; // Couldn't open file

    // Retrieve stream information
    if(av_find_stream_info(pFormatCtx)<0)
        return false; // Couldn't find stream information

    // Dump information about file onto standard error
    dump_format(pFormatCtx, 0, filename.toStdString().c_str(), false);

    // Find the first video stream
    videoStream=-1;
    for(unsigned i=0; i<pFormatCtx->nb_streams; i++)
        if(pFormatCtx->streams[i]->codec->codec_type==ffmpeg::AVMEDIA_TYPE_VIDEO)
        {
        videoStream=i;
        break;
    }
    if(videoStream==-1)
        return false; // Didn't find a video stream

    // Get a pointer to the codec context for the video stream
    pCodecCtx=pFormatCtx->streams[videoStream]->codec;

    // Find the decoder for the video stream
    pCodec=avcodec_find_decoder(pCodecCtx->codec_id);
    if(pCodec==NULL)
        return false; // Codec not found

    // Open codec
    if(avcodec_open(pCodecCtx, pCodec)<0)
        return false; // Could not open codec

    // Hack to correct wrong frame rates that seem to be generated by some
    // codecs
    if(pCodecCtx->time_base.num>1000 && pCodecCtx->time_base.den==1)
        pCodecCtx->time_base.den=1000;

    // Allocate video frame
    pFrame=ffmpeg::avcodec_alloc_frame();

    // Allocate an AVFrame structure
    pFrameRGB=ffmpeg::avcodec_alloc_frame();
    if(pFrameRGB==NULL)
        return false;

    // Determine required buffer size and allocate buffer
    numBytes=ffmpeg::avpicture_get_size(ffmpeg::PIX_FMT_RGB24, pCodecCtx->width,pCodecCtx->height);
    buffer=new uint8_t[numBytes];

    // Assign appropriate parts of buffer to image planes in pFrameRGB
    avpicture_fill((ffmpeg::AVPicture *)pFrameRGB, buffer, ffmpeg::PIX_FMT_RGB24, pCodecCtx->width, pCodecCtx->height);

    ok=true;
    return true;
}

bool QVideoDecoder::isOk()
{
    return ok;
}

bool QVideoDecoder::decodeSeekFrame(int after)
{
    if(!ok)
        return false;

    // If the last decoded frame satisfies the time condition we return it
    if( after!=-1 && ( LastFrameOk==true && after>=LastLastFrameNumber && after <= LastFrameNumber))
    {
        // This is the frame we want to return Compute desired frame time
        ffmpeg::AVRational millisecondbase = {1, 1000};
        DesiredFrameTime = ffmpeg::av_rescale_q(after,pFormatCtx->streams[videoStream]->time_base,millisecondbase);

        return true;
    }

    // The last decoded frame wasn't ok; either we need any new frame (after=-1), or a specific new frame with time>after
    bool done=false;
    while(!done)
    {
        // Read a frame
        if(av_read_frame(pFormatCtx, &packet)<0)
            return false;                             // Frame read failed (e.g. end of stream)

        if(packet.stream_index==videoStream)
        {
            // Is this a packet from the video stream -> decode video frame

            int frameFinished;
            avcodec_decode_video2(pCodecCtx,pFrame,&frameFinished,&packet);

            // Did we get a video frame?
            if(frameFinished)
            {
                ffmpeg::AVRational millisecondbase = {1, 1000};
                int f = packet.dts;
                int t = ffmpeg::av_rescale_q(packet.dts,pFormatCtx->streams[videoStream]->time_base,millisecondbase);
                if(LastFrameOk==false)
                {
                    LastFrameOk=true;
                    LastLastFrameTime=LastFrameTime=t;
                    LastLastFrameNumber=LastFrameNumber=f;
                }
                else
                {
                    // If we decoded 2 frames in a row, the last times are okay
                    LastLastFrameTime = LastFrameTime;
                    LastLastFrameNumber = LastFrameNumber;
                    LastFrameTime=t;
                    LastFrameNumber=f;
                }

                // Is this frame the desired frame?
                if(after==-1 || LastFrameNumber>=after)
                {
                    // Convert the image format (init the context the first time)
                    int w = pCodecCtx->width;
                    int h = pCodecCtx->height;
                    img_convert_ctx = ffmpeg::sws_getCachedContext(img_convert_ctx,w, h, pCodecCtx->pix_fmt, w, h, ffmpeg::PIX_FMT_RGB24, SWS_BICUBIC, NULL, NULL, NULL);

                    if(img_convert_ctx == NULL)
                    {
                        printf("Cannot initialize the conversion context!\n");
                        return false;
                    }
                    ffmpeg::sws_scale(img_convert_ctx, pFrame->data, pFrame->linesize, 0, pCodecCtx->height, pFrameRGB->data, pFrameRGB->linesize);

                    // Convert the frame to QImage
                    LastFrame=QImage(w,h,QImage::Format_RGB888);

                    for(int y=0;y<h;y++)
                        memcpy(LastFrame.scanLine(y),pFrameRGB->data[0]+y*pFrameRGB->linesize[0],w*3);

                    // Set the time
                    DesiredFrameTime = ffmpeg::av_rescale_q(after,pFormatCtx->streams[videoStream]->time_base,millisecondbase);
                    LastFrameOk=true;

                    done = true;
                }
            }
        }
        av_free_packet(&packet);
    }

    return done;   // done indicates whether or not we found a frame
}

bool QVideoDecoder::seekNextFrame()
{
    bool ret = decodeSeekFrame(DesiredFrameNumber+1);

    if(ret)
        DesiredFrameNumber++;   // Only updates the DesiredFrameNumber if we were successful in getting that frame
    else
        LastFrameOk=false;      // We didn't find the next frame (e.g. seek out of range) - mark we don't know where we are.
    return ret;
}

bool QVideoDecoder::seekMs(int tsms)
{
    if(!ok)
        return false;

    // Convert time into frame number
    DesiredFrameNumber = ffmpeg::av_rescale(tsms,pFormatCtx->streams[videoStream]->time_base.den,pFormatCtx->streams[videoStream]->time_base.num);
    DesiredFrameNumber/=1000;

    return seekFrame(DesiredFrameNumber);
}

bool QVideoDecoder::seekFrame(int64_t frame)
{

    if(!ok)
        return false;

    // Seek if:
    // - we don't know where we are (Ok=false)
    // - we know where we are but:
    //    - the desired frame is after the last decoded frame (this could be optimized: if the distance is small, calling decodeSeekFrame may be faster than seeking from the last key frame)
    //    - the desired frame is smaller or equal than the previous to the last decoded frame. Equal because if frame==LastLastFrameNumber we don't want the LastFrame, but the one before->we need to seek there
    if( (LastFrameOk==false) || ((LastFrameOk==true) && (frame<=LastLastFrameNumber || frame>LastFrameNumber) ) )
    {
        //printf("\t avformat_seek_file\n");
        if(ffmpeg::avformat_seek_file(pFormatCtx,videoStream,0,frame,frame,AVSEEK_FLAG_FRAME)<0)
            return false;

        avcodec_flush_buffers(pCodecCtx);

        DesiredFrameNumber = frame;
        LastFrameOk=false;
    }

    return decodeSeekFrame(frame);

    return true;
}

bool QVideoDecoder::getFrame(QImage&img,int *effectiveframenumber,int *effectiveframetime,int *desiredframenumber,int *desiredframetime)
{
    img = LastFrame;

    if(effectiveframenumber)
        *effectiveframenumber = LastFrameNumber;
    if(effectiveframetime)
        *effectiveframetime = LastFrameTime;
    if(desiredframenumber)
        *desiredframenumber = DesiredFrameNumber;
    if(desiredframetime)
        *desiredframetime = DesiredFrameTime;

    return LastFrameOk;
}

void QVideoDecoder::saveFramePPM(ffmpeg::AVFrame *pFrame, int width, int height, int iFrame)
{
    FILE *pFile;
    char szFilename[32];
    int  y;

    // Open file
    sprintf(szFilename, "frame%d.ppm", iFrame);
    pFile=fopen(szFilename, "wb");
    if(pFile==NULL)
        return;

    // Write header
    fprintf(pFile, "P6\n%d %d\n255\n", width, height);

    // Write pixel data
    for(y=0; y<height; y++)
        fwrite(pFrame->data[0]+y*pFrame->linesize[0], 1, width*3, pFile);

    // Close file
    fclose(pFile);
}

void QVideoDecoder::dumpFormat(ffmpeg::AVFormatContext *ic, int index, const char *url, int is_output)
{    
    uint8_t *printed = (uint8_t*)ffmpeg::av_mallocz(ic->nb_streams);
    if (ic->nb_streams && !printed)
        return;

    if (!is_output)
    {
        int hours, mins, secs, us;
        secs = ic->duration / AV_TIME_BASE;
        us = ic->duration % AV_TIME_BASE;
        mins = secs / 60;
        secs %= 60;
        hours = mins / 60;
        mins %= 60;
        //int secs, us;
        secs = ic->start_time / AV_TIME_BASE;
        us = ic->start_time % AV_TIME_BASE;
    }

    if(ic->nb_programs)
    {
        unsigned int j, total=0;
        for(j=0; j<ic->nb_programs; j++)
        {
            ffmpeg::AVMetadataTag *name = av_metadata_get(ic->programs[j]->metadata, "name", NULL, 0);
            total += ic->programs[j]->nb_stream_indexes;
        }        
    }

    if (ic->metadata)
    {
        ffmpeg::AVMetadataTag *tag=NULL;
    }

    ffmpeg::av_free(printed);
}

int QVideoDecoder::getVideoLengthMs()
{
    if(!isOk())
        return -1;

    int secs = pFormatCtx->duration / AV_TIME_BASE;
    int us = pFormatCtx->duration % AV_TIME_BASE;
    int l = secs*1000 + us/1000;

    dumpFormat(pFormatCtx,videoStream,"test video",0);

    return l;
}
